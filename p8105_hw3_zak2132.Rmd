---
title: "P8105: Homework #3"
author: "Zachary Katz (UNI: zak2132)"
date: "10/20/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(tidyverse)
library(lubridate)
library(viridis)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

First, we'd like to conduct some exploratory analysis of the `instacart` dataset. Our initial step is to load the data as follows:

```{r load instacart data into data frame}
library(p8105.datasets)
data("instacart")
```

Before answering the questions formulated as part of Problem 1, let's examine the tibble in its current form and make sure it's tidy.

```{r examine instacart df}
# Examine head and tail of `instacart` df
head(instacart) %>% knitr::kable()
tail(instacart) %>% knitr::kable()

# Examine structure, summary, and skim
str(instacart)
summary(instacart)
skimr::skim(instacart)
```

Instacart is an online service that allows grocery shopping from local stores. This data frame represents `r nrow(instacart)` observations, where each row in the tibble is a product from a given user's order. This dataset has already been cleaned such that there is only one order per user included here. In total, this data contains product order information for `r length(unique(pull(instacart, user_id)))` unique users.

For each observation, `r ncol(instacart)` variables are represented; our examination of the data set above indicates that there are no missing values. The full set of variables included is: `r names(instacart)`. Each of these variables operates as follows:

`order_id`, `product_id`, `user_id`, `product_name`, `department_id`, `department`, `aisle_id`, and `aisle` are identifiers of the order, product, product's department, and user. `add_to_cart_order`, `order_number`, `order_dow`, `order_hour_of_day`, `days_since_prior_order` indicate when orders were placed in the cart and ordered both absolutely in time and relative to other orders. `reordered` is an indicator that is 1 if a product has been ordered by the user in the past, while `eval_set` indicates that all orders in this data set are part of the "train" evaluation set.

Let's clean our tibble to be a bit clearer:

```{r clean instacart df}
instacart = instacart %>% 
      # Remove and rearrange certain columns
      select(-eval_set) %>% 
      select (order_id, product_id, user_id, everything()) %>% 
      # Recode certain variables
      mutate(reordered = as.logical(reordered))
```

Now, we can start answer questions like: How many aisles are there, and which aisles are the most items ordered from?

To determine the number of unique aisles, we calculate as follows:

```{r calculate unique aisles}
num_aisles = length(unique(pull(instacart, aisle)))
```

There are `r num_aisles` in total.

Now, let's see which ones have the most items ordered from them. To determine the aisles with the most items ordered from them, we want to sort the aisles in descending order of observations in that aisle. 

```{r calculate aisles with highest number of items ordered}
# Group by aisle and summarize number of observations in descending order
orders_by_aisle = instacart %>% 
      group_by(aisle) %>% 
      summarize(n_obs = n()) %>% 
      arrange(desc(n_obs)) 

# Check out the top 5 aisles by number of observations in table form
orders_by_aisle %>% 
      head(5) %>% 
      knitr::kable()
```

From the table, we can tell that fresh vegetables and fresh fruits are the two aisles with the most items ordered, with over 150,000 observations each.

We can create a plot to show the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered.

```{r create plot of number of items ordered by aisle}
orders_by_aisle %>% 
      # Filter observations
      filter(n_obs > 10000) %>% 
      # Relevel aisles as factor by number of observations
      mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>% 
      ggplot(aes(
            x = aisle,
            y = n_obs)
      ) + 
      geom_bar(stat = "identity") + 
      # Flip X and Y axes
      coord_flip() + 
      # Label graph
      labs(
            title = "Number of items ordered per aisle",
            y = "Number of items ordered",
            x = "Product aisle",
            caption = "Note: only includes aisles with >10K items ordered"
      )
```

Additionally, we'd like to create a table showing the three most popular items in each of the following aisles: "baking ingredients," "dog food care," and "packaged vegetables fruits." We should also include in our table the number of times each item is ordered.

```{r table showing most popular items in given aisles}
instacart %>% 
      # Filter for appropriate aisles and group by aisle and product name
      filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
      group_by(aisle, product_name) %>% 
      summarize(n_obs = n()) %>% 
      # Determine rank for product in each aisle by number of observations
      mutate(
            product_rank = min_rank(desc(n_obs))
      ) %>% 
      filter(product_rank < 4) %>% 
      # Arrange table by aisle and then product rank by # of obs
      arrange(aisle, product_rank) %>% 
      rename(num_orders = n_obs) %>% 
      knitr::kable()
```

Finally, let's create a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. Let's have one row for apples and another for coffee, and seven columns representing each day of the week. The value for each cell should represent the mean hour of the day in which the product is ordered.

```{r table showing mean hour for apples and coffee}
instacart %>%
      # Select pink lady apples and coffee ice cream, then group by product and day of week
      select(product_name, order_dow, order_hour_of_day) %>% 
      filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
      group_by(product_name, order_dow) %>% 
      # Summarize mean for each group
      summarize(
            mean_hour = mean(order_hour_of_day)
      ) %>% 
      pivot_wider(
            names_from = "order_dow",
            values_from = "mean_hour"
      ) %>%
      # Recode column names as days of the week
      rename(
            "Sunday" = "0",
            "Monday" = "1",
            "Tuesday" = "2",
            "Wednesday" = "3",
            "Thursday" = "4",
            "Friday" = "5",
            "Saturday" = "6"
      ) %>% 
      knitr::kable()
```

## Problem 2

For problem 2, we use BRFSS data, so let's load it in and check it out.

```{r load and examine brfss data}
# Load the data
data("brfss_smart2010")

# Examine head and tail of `brfss_smart2010` df
head(brfss_smart2010) %>% knitr::kable()
tail(brfss_smart2010) %>% knitr::kable()

# Examine structure, summary, and skim
str(brfss_smart2010)
summary(brfss_smart2010)
skimr::skim(brfss_smart2010)
```

The BRFSS data set, or Behavioral Risk Factors Surveillance System, is intended to enable analysis of metropolitan area risk trends for selected counties. Data is collected from a continuous system that tracks modifiable risk factors for chronic diseases and other leading causes of death.

Our data set contains `r nrow(brfss_smart2010)` observations and `r ncol(brfss_smart2010)` variables. Let's start to tidy it up a bit and drill down towards what we're interested in examining.

```{r tidy the brfss data}
brfss_smart2010 = brfss_smart2010 %>% 
      # Clean variable names
      janitor::clean_names() %>% 
      rename(
            "state" = "locationabbr",
            "county" = "locationdesc"
      ) %>% 
      # Drop columns full of missing values or that do not provide information of value
      select(-data_value_footnote_symbol, -data_value_footnote, -location_id, -data_source) %>% 
      # Focus on "Overall Health" topic
      filter(topic == "Overall Health") %>% 
      # Include only responses from "Excellent" to "Poor" %>% 
      filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
      # Organize responses as a factor taking levels ordered from "Poor" to "Excellent"
      mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

Having tidied our data set a bit, we can begin to answer a few questions. For example, in 2002, which states were observed at 7 or more locations? What about in 2010?

```{r states with 7+ observations}
# Filter for 2002, group by state, and sum the number of unique locations observed
brfss_smart2010 %>% 
      filter(year == 2002) %>% 
      group_by(state) %>% 
      summarize(locations_observed = length(unique(county))) %>% 
      filter(locations_observed >= 7) %>% 
      arrange(desc(locations_observed)) %>% 
      knitr::kable()

# Repeat for 2010
brfss_smart2010 %>% 
      filter(year == 2010) %>% 
      group_by(state) %>% 
      summarize(locations_observed = length(unique(county))) %>% 
      filter(locations_observed >= 7) %>% 
      arrange(desc(locations_observed)) %>% 
      knitr::kable()
```

The states that were observed at 7 or more locations in 2002 were PA, MA, NJ, CT, FL, and NC. The states that were observed at 7 or more locations in 2010 were FL, NJ, TX, CA, MD, NC, NE, WA, MA, NY, OH, CO, PA, and SC -- significantly more than in 2002!

We'd also like to get a better understanding of how the average `data_value` across locations within each state changed over time for responses labeled as "Excellent."

```{r spaghetti plot change in data_value over time by state}
brfss_smart2010 %>% 
      # Filter and group
      filter(response == "Excellent") %>% 
      group_by(state, year) %>% 
      # Create new summary measure
      summarize(
            mean_data_value = mean(data_value)
      ) %>% 
      # Develop spaghetti plot
      ggplot(aes(x = year, y = mean_data_value, color = state)) + 
      geom_line(alpha = 0.4) + 
      geom_point(alpha = 0.4) + 
      labs(
            title = "Average data value for excellent responses over time by state",
            x = "Year",
            y = "Mean data value"
      ) + 
      theme(legend.position = "right")
```

Finally, we can also create a two-panel plot showing, for 2006 and 2010, the distribution of `data_value` for responses ("Poor" to "Excellent") among locations in the state of New York.

```{r Two-panel plot showing distribution of data value for NYS}
brfss_smart2010 %>% 
      filter(
            year == 2006 | year == 2010,
            state == "NY"
      ) %>% 
      # Choose necessary columns
      select(year, response, data_value) %>% 
      ggplot(aes(x = response, y = data_value)) + 
      geom_boxplot(aes(fill = response), alpha = 0.5) + 
      theme(legend.position = "none") + 
      facet_grid(~year) + 
      labs(
            title = "Distribution of `data_value` by response in NY, 2006 vs 2010",
            x = "Response category",
            y = "Data Value"
      )
```

There are a couple of interesting trends observable in this plot. The first is the backwards-bending curve which shows "excellent" responses generally receiving lower data value scores than "good" and "very good" responses. Another is the general increase in data value from 2006 and 2010 for the "very good" response category in New York state.
