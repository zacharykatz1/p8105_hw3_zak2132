---
title: "P8105: Homework #3"
author: "Zachary Katz (UNI: zak2132)"
date: "10/20/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(tidyverse)
library(lubridate)
library(viridis)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

First, we'd like to conduct some exploratory analysis of the `instacart` dataset. Our initial step is to load the data as follows:

```{r load instacart data into data frame}
library(p8105.datasets)
data("instacart")
```

Before answering the questions formulated as part of Problem 1, let's examine the tibble in its current form and make sure it's tidy.

```{r examine instacart df}
# Examine head and tail of `instacart` df
head(instacart) %>% knitr::kable()
tail(instacart) %>% knitr::kable()

# Examine structure, summary, and skim
str(instacart)
summary(instacart)
skimr::skim(instacart)
```

Instacart is an online service that allows grocery shopping from local stores. This data frame represents `r nrow(instacart)` observations, where each row in the tibble is a product from a given user's order. This dataset has already been cleaned such that there is only one order per user included here. In total, this data contains product order information for `r length(unique(pull(instacart, user_id)))` unique users.

For each observation, `r ncol(instacart)` variables are represented; our examination of the data set above indicates that there are no missing values. The full set of variables included is: `r names(instacart)`. Each of these variables operates as follows:

`order_id`, `product_id`, `user_id`, `product_name`, `department_id`, `department`, `aisle_id`, and `aisle` are identifiers of the order, product, product's department, and user. `add_to_cart_order`, `order_number`, `order_dow`, `order_hour_of_day`, `days_since_prior_order` indicate when orders were placed in the cart and ordered both absolutely in time and relative to other orders. `reordered` is an indicator that is 1 if a product has been ordered by the user in the past, while `eval_set` indicates that all orders in this data set are part of the "train" evaluation set.

Let's clean our tibble to be a bit clearer:

```{r clean instacart df}
instacart = instacart %>% 
      # Remove and rearrange certain columns
      select(-eval_set) %>% 
      select (order_id, product_id, user_id, everything()) %>% 
      # Recode certain variables
      mutate(reordered = as.logical(reordered))
```

Now, we can start answer questions like: How many aisles are there, and which aisles are the most items ordered from?

To determine the number of unique aisles, we calculate as follows:

```{r calculate unique aisles}
num_aisles = length(unique(pull(instacart, aisle)))
```

There are `r num_aisles` aisles in total.

Now, let's see which ones have the most items ordered from them. To determine the aisles with the most items ordered from them, we want to sort the aisles in descending order of observations in that aisle. 

```{r calculate aisles with highest number of items ordered}
# Group by aisle and summarize number of observations in descending order
orders_by_aisle = instacart %>% 
      group_by(aisle) %>% 
      summarize(aisle_obs = n()) %>% 
      arrange(desc(aisle_obs)) 

# Check out the top 5 aisles by number of observations in table form
orders_by_aisle %>% 
      head(5) %>% 
      knitr::kable()
```

From the table, we can tell that fresh vegetables and fresh fruits are the two aisles with the most items ordered, with over 150,000 observations each.

We can create a plot to show the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered.

```{r create plot of number of items ordered by aisle}
orders_by_aisle %>% 
      # Filter observations
      filter(aisle_obs > 10000) %>% 
      # Relevel aisles as factor by number of observations
      mutate(aisle = forcats::fct_reorder(aisle, aisle_obs)) %>% 
      ggplot(aes(
            x = aisle,
            y = aisle_obs)
      ) + 
      geom_bar(stat = "identity", position = position_dodge(width = 0.9), width= 0.7) + 
      # Flip X and Y axes
      coord_flip() + 
      # Label graph
      scale_fill_gradient(low = "grey", high = "black") + 
      labs(
            title = "Number of items ordered per aisle",
            y = "Number of items ordered",
            x = "Product aisle",
            caption = "Note: only includes aisles with >10K items ordered"
      )
```

Additionally, we'd like to create a table showing the three most popular items in each of the following aisles: "baking ingredients," "dog food care," and "packaged vegetables fruits." We should also include in our table the number of times each item is ordered.

```{r table showing most popular items in given aisles, message=FALSE}
instacart %>% 
      # Filter for appropriate aisles and group by aisle and product name
      filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
      group_by(aisle, product_name) %>% 
      summarize(n_obs = n()) %>% 
      # Determine rank for product in each aisle by number of observations
      mutate(
            product_rank = min_rank(desc(n_obs))
      ) %>% 
      filter(product_rank < 4) %>% 
      # Arrange table by aisle and then product rank by # of obs
      arrange(aisle, product_rank) %>% 
      rename(num_orders = n_obs) %>% 
      knitr::kable()
```

Finally, let's create a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. Let's have one row for apples and another for coffee, and seven columns representing each day of the week. The value for each cell should represent the mean hour of the day in which the product is ordered.

```{r table showing mean hour for apples and coffee, message=FALSE}
instacart %>%
      # Select pink lady apples and coffee ice cream, then group by product and day of week
      select(product_name, order_dow, order_hour_of_day) %>% 
      filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
      group_by(product_name, order_dow) %>% 
      # Summarize mean for each group
      summarize(
            mean_hour = round(mean(order_hour_of_day), 2)
      ) %>% 
      pivot_wider(
            names_from = "order_dow",
            values_from = "mean_hour"
      ) %>%
      # Recode column names as days of the week
      rename(
            "Sunday" = "0",
            "Monday" = "1",
            "Tuesday" = "2",
            "Wednesday" = "3",
            "Thursday" = "4",
            "Friday" = "5",
            "Saturday" = "6"
      ) %>% 
      knitr::kable()
```

## Problem 2

For problem 2, we use BRFSS data, so let's load it in and check it out.

```{r load and examine brfss data}
# Load the data
data("brfss_smart2010")

# Examine head and tail of `brfss_smart2010` df
head(brfss_smart2010) %>% knitr::kable()
tail(brfss_smart2010) %>% knitr::kable()

# Examine structure, summary, and skim
str(brfss_smart2010)
summary(brfss_smart2010)
skimr::skim(brfss_smart2010)
```

The BRFSS data set, or Behavioral Risk Factors Surveillance System, is intended to enable analysis of metropolitan area risk trends for selected counties. Data is collected from a continuous system that tracks modifiable risk factors for chronic diseases and other leading causes of death.

Our data set contains `r nrow(brfss_smart2010)` observations and `r ncol(brfss_smart2010)` variables. Let's start to tidy it up a bit and drill down towards what we're interested in examining.

```{r tidy the brfss data}
brfss_smart2010 = brfss_smart2010 %>% 
      # Clean variable names
      janitor::clean_names() %>% 
      rename(
            "state" = "locationabbr",
            "county" = "locationdesc"
      ) %>% 
      # Drop columns full of missing values or that do not provide information of value
      select(-data_value_footnote_symbol, -data_value_footnote, -location_id, -data_source) %>% 
      # Focus on "Overall Health" topic
      filter(topic == "Overall Health") %>% 
      # Include only responses from "Excellent" to "Poor" %>% 
      filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
      # Organize responses as a factor taking levels ordered from "Poor" to "Excellent"
      mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  distinct()
```

Having tidied our data set a bit, we can begin to answer a few questions. For example, in 2002, which states were observed at 7 or more locations? What about in 2010?

```{r states with 7+ observations}
# Filter for 2002, group by state, and sum the number of unique locations observed
brfss_smart2010 %>% 
      filter(year == 2002) %>% 
      group_by(state) %>% 
      summarize(locations_observed = length(unique(county))) %>% 
      filter(locations_observed >= 7) %>% 
      arrange(desc(locations_observed)) %>% 
      knitr::kable()

# Repeat for 2010
brfss_smart2010 %>% 
      filter(year == 2010) %>% 
      group_by(state) %>% 
      summarize(locations_observed = length(unique(county))) %>% 
      filter(locations_observed >= 7) %>% 
      arrange(desc(locations_observed)) %>% 
      knitr::kable()
```

The six states that were observed at 7 or more locations in 2002 were PA, MA, NJ, CT, FL, and NC. The 14 states that were observed at 7 or more locations in 2010 were FL, NJ, TX, CA, MD, NC, NE, WA, MA, NY, OH, CO, PA, and SC -- significantly more than in 2002!

We'd also like to get a better understanding of how the average `data_value` across locations within each state changed over time for responses labeled as "Excellent."

```{r spaghetti plot change in data_value over time by state, message=FALSE}
brfss_smart2010 %>% 
      # Filter and group
      filter(response == "Excellent") %>% 
      group_by(state, year) %>% 
      # Create new summary measure
      summarize(
            mean_data_value = mean(data_value)
      ) %>% 
      # Develop spaghetti plot
      ggplot(aes(x = year, y = mean_data_value, group = state, color = state)) + 
      geom_line(alpha = 0.5, size=2) + 
      geom_point(alpha = 0.5) + 
      labs(
            title = "Average score for excellents by state",
            x = "Year",
            y = "Mean data value"
      ) + 
      theme(legend.position = "right")
```

The mean data value fluctuates quite a bit over time and across states. West Virginia is a notable outlier, with lower average scores over time, especially in 2005 and 2009, compared to other states.

I'm a bit curious how NY looks compared to the others on this metric, so I'll also create a graph that highlights NY's performance in particular.

```{r}
# Group for all states
all_states = brfss_smart2010 %>% 
      # Filter and group
      filter(response == "Excellent") %>% 
      group_by(state, year) %>% 
      # Create new summary measure
      summarize(
            mean_data_value = mean(data_value)
      ) 

# Filter for NY specifically
ny_only = all_states %>% 
  filter(state == "NY")

 # Develop plot
ggplot() + 
  geom_line(data = all_states, aes(x = year, y = mean_data_value, group = state), color = alpha("grey", 0.7)) + 
  geom_line(data = ny_only, aes(x = year, y = mean_data_value, color = state), color = alpha("blue", 1.0), size = 2) + 
      labs(
            title = "Average score, NY vs others",
            x = "Year",
            y = "Mean data value"
      )
```

Looks like New York is right around the middle of the pack.

Finally, we can also create a two-panel plot showing, for 2006 and 2010, the distribution of `data_value` for responses ("Poor" to "Excellent") among locations in the state of New York.

```{r Two-panel plot showing distribution of data value for NYS}
brfss_smart2010 %>% 
      filter(
            year == 2006 | year == 2010,
            state == "NY"
      ) %>% 
      # Choose necessary columns
      select(year, response, data_value) %>% 
      ggplot(aes(x = response, y = data_value)) + 
      geom_boxplot(aes(fill = response), alpha = 0.5) + 
      theme(legend.position = "none") + 
      facet_grid(~year) + 
      labs(
            title = "Distribution of `data_value` by response in NY, 2006 vs 2010",
            x = "Response category",
            y = "Data Value"
      )
```

There are a couple of interesting trends observable in this plot. The first is the backwards-bending curve which shows "excellent" responses generally receiving lower data value scores than "good" and "very good" responses. Another is the general increase in data value from 2006 and 2010 for the "very good" response category in New York state.

In addition, we can visualize this in a couple of other ways as well, just for fun: in the following, the first method looks at density across all responses without factorizing by response, while the second method looks at density once we factorize, or group, by response.

```{r other NY viz}
# Density plot without group factors
brfss_smart2010 %>% 
      filter(
            year == 2006 | year == 2010,
            state == "NY"
      ) %>% 
  group_by(year) %>% 
  ggplot(aes(x = data_value)) + 
  geom_density() + 
  facet_grid(~year)

# Density plot with group factors
brfss_smart2010 %>% 
      filter(
            year == 2006 | year == 2010,
            state == "NY"
      ) %>% 
  group_by(year) %>% 
  ggplot(aes(x = data_value, group = response, color = response)) + 
  geom_density() + 
  facet_grid(~year)
```

In this case, we find the extra grouping by response category to be more informative once again. In general, as we move from "poor" to "excellent," the density also tends to shift towards higher values of `data_value` -- except for "excellent" responses, which revert back towards middling values of `data_value`. From 2006 to 2010, this trend is maintained, although we generally see a very slight increase in density for higher values of `data_value`.

## Problem 3

Our goal in problem 3 is to load, tidy, wrangle, and explore five weeks of accelerometer data collected from an individual admitted to the hospital and diagnosed with congestive heart failure.

As always, let's first load in the data.

```{r load accelerometer data}
accel_df = read_csv("Data/accel_data.csv") %>% 
      janitor::clean_names()
```

Wow! This sure is a wide dataset, with `r ncol(accel_df) variables. As it turns out, we need to tidy the data so that each row represents one observation, i.e. one minute for this patient on any given date. We can pivot longer to do that, and can also clean up the data in a few other ways.

```{r tidy accelerometer data}
accel_df = accel_df %>% 
      # Pivot data
      pivot_longer(
            activity_1:activity_1440,
            names_to = "minute_id",
            names_prefix = "activity_",
            values_to = "activity_count"
      ) %>% 
      # Rename variables
      rename(
            "week_id" = "week",
            "day_of_week" = "day") %>% 
      # Include weekday vs. weekend variable
      mutate(
            "type_of_day" = 
                  ifelse(
                        day_of_week %in% c("Saturday", "Sunday"), "Weekend",
                        "Weekday"
                  )
      ) %>% 
      # Reorganize variables
      select(week_id, day_id, minute_id, day_of_week, type_of_day, activity_count) %>% 
      # Encode reasonable variable classes
      mutate(
            minute_id = as.integer(minute_id),
            day_of_week = as.factor(day_of_week),
            type_of_day = as.factor(type_of_day)
      )
```

Our tidied tibble has `r nrow(accel_df)` observations, one representing each minute over the five weeks (35 days) of data collection. In total, there are `r ncol(accel_df)` variables, with `week_id` indexing the week from 1 to 5 in which the data was collected, `day_id` indexing the day from 1 to 35 in which the data was collected, `minute_id` indexing the minute from 1 to 50,400 in which the data was collected, `day_of_week` indicating the day of the week the observation was made, `type_of_day` encoding whether the day of week is a weekend or weekday, and `activity_count` as the outcome variable of interest.

We can also take a quick look at the data to make sure everything looks right:

```{r brief look at the accelerometer tidied data}
head(accel_df) %>% knitr::kable()
tail(accel_df) %>% knitr::kable()

# Examine structure, summary, and skim
str(accel_df)
summary(accel_df)
skimr::skim(accel_df)
```

Everything looks as expected! Now let's aggregate across minutes to create a total activity variable for each day, and then create a table showing these totals.

```{r total daily activity table}
total_daily = accel_df %>% 
      group_by(day_id) %>% 
      summarize(
            total_daily_activity = sum(activity_count)
      )

# Don't pipe in, because will reuse `total_daily` in a little while
total_daily %>% 
      knitr::kable()
```

We note that on some days, such as on days 24 and 31, the total_daily_activity is much lower than on other days. But we get this just by eyeballing the table; can we construct another summary column indicating how far each day's total activity deviates from the mean across days?

```{r daily activity with deviation from mean}
# Create a column that normalizes on `total_daily_activity`
total_daily_deviation = total_daily %>% 
      mutate(
            deviation = (total_daily_activity - mean(total_daily_activity)) / sd(total_daily_activity)) %>% 
      filter(abs(deviation) > 1.5) %>% 
  knitr::kable()
```

We see here a couple of outliers -- namely, days 24 and 31 -- that are 2.34 standard deviations below the mean. Interestingly, the `total_daily_activity` on these days is identical, at 1440. Notably, we know from the original tibble that days 24 and 31 are Saturdays. Day 2 is also pretty low; that's a Monday.

We can also check to see if weekend days have a generally lower daily activity average than weekdays, and indeed they do.

```{r average daily activity weekend vs weekday}
# Create table of average daily activity, grouping by weekday or weekend
accel_df %>% 
      group_by(type_of_day) %>% 
      summarize(
           average_daily_activity = mean(activity_count)
      ) %>% 
      knitr::kable()
```

Finally, let's make a plot that shows the 24-hour activity time course for each day.

```{r 24h time course per day}
# Create spaghetti plot showing 24h activity by minute for each day_id, colored by day of week
ggplot(data = accel_df, aes(x = minute_id, y = activity_count, group = day_id, color = day_of_week)) + 
      geom_line(alpha = 0.5) + 
      geom_point(alpha = 0.5) + 
      labs(
            x = "Minute of day",
            y = "Activity count",
            title = "Activity by minute per day",
            color = "Day of week"
      ) + 
      theme(legend.position = "right")
```

It's difficult to discern distinct patterns or draw certain conclusions from this graph, which shows perhaps too many data points. We could possibly say that this individual has an unusually high amount of activity on Friday evenings (~1250 minutes into the day, which is between 8pm and 9pm) and on Sunday around 800 minutes into the day (about 1-2pm), as well as an unusually low amount of activity every day of the week until several hours into the morning (~400 minutes into the day, which is between 6am and 7am). 

As an aside, I would prefer to group by day of week first and then find the mean at a given minute during the day, and plot that over time, like so:

```{r 24h time course but grouped and averaged by day of week, message=FALSE}
accel_df %>% 
      group_by(day_of_week, minute_id) %>% 
      summarize(
            mean_per_minute = mean(activity_count)
      ) %>% 
      ggplot(aes(x = minute_id, y = mean_per_minute, color = day_of_week)) + 
      geom_line(alpha = 0.8) +
      labs(
            x = "Minute of day",
            y = "Mean activity count",
            title = "Activity by minute grouped by day of week",
            color = "Day of week"
      ) + 
      theme(legend.position = "right")
```


